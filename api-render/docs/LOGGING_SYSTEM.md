# Reddit Scraper Logging System

â”Œâ”€ LOGGING ARCHITECTURE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â— COMPREHENSIVE â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% COVERAGE   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## Executive Summary

```json
{
  "status": "FULLY_IMPLEMENTED",
  "coverage": "100%",
  "destination": "system_logs table",
  "source": "reddit_scraper",
  "components": [
    "simple_main.py",
    "continuous_v3.py",
    "calculator.py",
    "api_pool.py"
  ],
  "log_levels": ["debug", "info", "success", "warning", "error"],
  "retention": "30 days"
}
```

## Log Levels and Usage

### Log Level Guidelines

```json
{
  "debug": {
    "description": "Detailed debugging information",
    "examples": [
      "API request details",
      "Metric calculation inputs/outputs",
      "Subreddit about data retrieved"
    ],
    "when_to_use": "Development and troubleshooting"
  },
  "info": {
    "description": "Normal operational events",
    "examples": [
      "Processing subreddit start",
      "Cycle start/end",
      "User processing count"
    ],
    "when_to_use": "Key workflow milestones"
  },
  "success": {
    "description": "Successful completion events",
    "examples": [
      "Subreddit processing complete",
      "Cycle complete with stats",
      "Database operations successful"
    ],
    "when_to_use": "Completion of major operations"
  },
  "warning": {
    "description": "Non-critical issues",
    "examples": [
      "No weekly posts available",
      "Rate limiting encountered",
      "Slow API requests (>5s)"
    ],
    "when_to_use": "Degraded but functional state"
  },
  "error": {
    "description": "Failures requiring attention",
    "examples": [
      "API request failures",
      "Database operation errors",
      "Initialization failures"
    ],
    "when_to_use": "Operation failures"
  }
}
```

## Context Field Standards

### Standard Context Fields

```json
{
  "common_fields": {
    "action": "The specific action being performed",
    "subreddit": "Subreddit name when applicable",
    "thread_id": "Thread identifier for concurrent operations",
    "duration_ms": "Operation duration in milliseconds",
    "error": "Error message when applicable"
  },
  "component_specific": {
    "simple_main": {
      "cycle_id": "Unique cycle identifier",
      "type": "Subreddit type (Ok/No Seller)",
      "metrics": "Calculated metrics object",
      "verification_required": "Boolean flag",
      "posts_saved": "Number of posts saved",
      "users_analyzed": "Number of users processed"
    },
    "calculator": {
      "posts_analyzed": "Number of posts used",
      "result": "Calculation result",
      "engagement": "Engagement value",
      "subreddit_score": "Score value"
    },
    "api_pool": {
      "url": "Full request URL",
      "status_code": "HTTP response code",
      "response_time_ms": "Request duration",
      "retry": "Retry attempt number"
    }
  }
}
```

## Key Logging Points

### 1. Initialization & Cleanup

```python
# Scraper initialization
log('info', 'ðŸš€ Initializing Reddit Scraper',
    {'version': SCRAPER_VERSION, 'action': 'initialization_start'})

# Successful initialization
log('success', 'âœ… Scraper initialization complete',
    {'duration_seconds': duration, 'proxy_count': count})

# Initialization failure
log('error', 'âŒ Failed to initialize scraper',
    {'error': str(e), 'action': 'initialization_failed'})
```

### 2. Cycle Management

```python
# Cycle start
log('info', 'ðŸ”„ Starting scraping cycle',
    {'cycle_id': cycle_id, 'action': 'cycle_start'})

# Cycle completion
log('success', 'âœ… Scraping cycle complete',
    {'cycle_id': cycle_id, 'subreddits_processed': count,
     'duration_seconds': duration})
```

### 3. Subreddit Processing

```python
# Processing start
log('info', f'Processing {type} subreddit: r/{name}',
    {'subreddit': name, 'type': type, 'action': 'processing_start'})

# Processing complete
log('success', f'âœ… Completed processing for r/{name}',
    {'subreddit': name, 'metrics': metrics, 'posts_saved': count})

# Processing error
log('error', f'Error processing subreddit {name}',
    {'subreddit': name, 'error': str(e), 'action': 'processing_failed'})
```

### 4. Metrics Calculation

```python
# Engagement calculation
log('debug', f'Calculated engagement: {engagement:.6f}',
    {'action': 'engagement_calculated', 'posts_analyzed': count,
     'total_comments': comments, 'total_upvotes': upvotes})

# No weekly posts warning
log('warning', 'No weekly posts for metrics calculation',
    {'action': 'metrics_fallback', 'metrics_set_to_zero': True})

# Subreddit score
log('debug', f'Calculated subreddit_score: {score:.2f}',
    {'action': 'score_calculated', 'engagement': engagement,
     'avg_upvotes': upvotes})
```

### 5. API Operations

```python
# Rate limiting
log('warning', 'Rate limited by Reddit',
    {'url': url, 'status_code': 429, 'retry': attempt, 'delay': delay})

# Slow requests
log('warning', f'Slow API request: {response_time_ms}ms',
    {'url': url, 'response_time_ms': time, 'action': 'slow_request'})

# Request failures
log('error', f'Request failed after {max_retries} retries',
    {'url': url, 'max_retries': retries, 'error': str(e)})
```

### 6. Database Operations

```python
# Successful saves
log('debug', f'ðŸ’¾ Saved {count} posts from r/{subreddit}',
    {'subreddit': subreddit, 'posts_count': count, 'action': 'posts_saved'})

# Save errors
log('error', 'Error saving posts batch',
    {'subreddit': subreddit, 'batch_size': size, 'error': str(e)})
```

## Monitoring Queries

### 1. Recent Errors (Last Hour)

```sql
SELECT
    timestamp,
    message,
    context->>'subreddit' as subreddit,
    context->>'error' as error
FROM system_logs
WHERE source = 'reddit_scraper'
  AND level = 'error'
  AND timestamp > NOW() - INTERVAL '1 hour'
ORDER BY timestamp DESC
LIMIT 50;
```

### 2. Subreddit Processing Performance

```sql
SELECT
    DATE_TRUNC('hour', timestamp) as hour,
    context->>'type' as subreddit_type,
    COUNT(*) as processed,
    AVG((context->>'duration_ms')::INT) as avg_duration_ms
FROM system_logs
WHERE source = 'reddit_scraper'
  AND message LIKE '%Completed%processing%'
  AND timestamp > NOW() - INTERVAL '24 hours'
GROUP BY 1, 2
ORDER BY 1 DESC;
```

### 3. Metrics Calculation Issues

```sql
SELECT
    timestamp,
    context->>'subreddit' as subreddit,
    message,
    context->>'engagement' as engagement,
    context->>'subreddit_score' as score
FROM system_logs
WHERE source = 'reddit_scraper'
  AND script_name = 'calculator'
  AND (
    message LIKE '%No weekly posts%'
    OR message LIKE '%set to 0%'
  )
  AND timestamp > NOW() - INTERVAL '24 hours'
ORDER BY timestamp DESC;
```

### 4. API Performance Monitoring

```sql
-- Rate limiting patterns
SELECT
    DATE_TRUNC('hour', timestamp) as hour,
    COUNT(*) as rate_limits,
    AVG((context->>'delay')::INT) as avg_delay_seconds
FROM system_logs
WHERE source = 'reddit_scraper'
  AND context->>'action' = 'rate_limit'
  AND timestamp > NOW() - INTERVAL '24 hours'
GROUP BY 1
ORDER BY 1 DESC;

-- Slow API requests
SELECT
    timestamp,
    context->>'url' as url,
    context->>'response_time_ms' as response_time_ms,
    context->>'thread_id' as thread
FROM system_logs
WHERE source = 'reddit_scraper'
  AND context->>'action' = 'slow_request'
  AND timestamp > NOW() - INTERVAL '1 hour'
ORDER BY (context->>'response_time_ms')::INT DESC
LIMIT 20;
```

### 5. Cycle Performance Overview

```sql
WITH cycle_stats AS (
    SELECT
        context->>'cycle_id' as cycle_id,
        MIN(timestamp) as start_time,
        MAX(timestamp) as end_time,
        MAX((context->>'subreddits_processed')::INT) as subreddits,
        MAX((context->>'posts_processed')::INT) as posts,
        MAX((context->>'users_processed')::INT) as users,
        MAX((context->>'duration_seconds')::FLOAT) as duration
    FROM system_logs
    WHERE source = 'reddit_scraper'
      AND context->>'cycle_id' IS NOT NULL
      AND timestamp > NOW() - INTERVAL '24 hours'
    GROUP BY 1
)
SELECT
    cycle_id,
    start_time,
    duration as duration_seconds,
    subreddits,
    posts,
    users,
    ROUND(posts::NUMERIC / NULLIF(duration, 0), 2) as posts_per_second
FROM cycle_stats
ORDER BY start_time DESC
LIMIT 10;
```

### 6. Data Quality Monitoring

```sql
-- Subreddits with zero metrics
SELECT
    rs.name,
    rs.review,
    rs.engagement,
    rs.subreddit_score,
    rs.avg_upvotes_per_post,
    rs.last_scraped_at,
    COUNT(sl.id) as log_entries
FROM reddit_subreddits rs
LEFT JOIN system_logs sl ON
    sl.context->>'subreddit' = rs.name
    AND sl.timestamp > NOW() - INTERVAL '48 hours'
WHERE rs.review = 'Ok'
  AND (rs.engagement = 0 OR rs.engagement IS NULL)
  AND rs.last_scraped_at > NOW() - INTERVAL '7 days'
GROUP BY rs.name, rs.review, rs.engagement, rs.subreddit_score,
         rs.avg_upvotes_per_post, rs.last_scraped_at
ORDER BY rs.last_scraped_at DESC;
```

### 7. Scraper Health Dashboard

```sql
-- Overall health metrics
SELECT
    COUNT(CASE WHEN level = 'error' THEN 1 END) as errors,
    COUNT(CASE WHEN level = 'warning' THEN 1 END) as warnings,
    COUNT(CASE WHEN level = 'success' THEN 1 END) as successes,
    COUNT(CASE WHEN context->>'action' = 'rate_limit' THEN 1 END) as rate_limits,
    COUNT(CASE WHEN context->>'action' = 'slow_request' THEN 1 END) as slow_requests,
    MAX(timestamp) as last_activity
FROM system_logs
WHERE source = 'reddit_scraper'
  AND timestamp > NOW() - INTERVAL '1 hour';
```

## Alert Recommendations

### Critical Alerts

```sql
-- No activity in 30 minutes
SELECT
    CASE
        WHEN MAX(timestamp) < NOW() - INTERVAL '30 minutes'
        THEN 'ALERT: Scraper appears to be stopped'
        ELSE 'OK: Scraper active'
    END as status,
    MAX(timestamp) as last_activity,
    EXTRACT(EPOCH FROM (NOW() - MAX(timestamp))) / 60 as minutes_since_activity
FROM system_logs
WHERE source = 'reddit_scraper';

-- High error rate
SELECT
    DATE_TRUNC('hour', timestamp) as hour,
    COUNT(CASE WHEN level = 'error' THEN 1 END) as errors,
    COUNT(*) as total,
    ROUND(COUNT(CASE WHEN level = 'error' THEN 1 END)::NUMERIC / COUNT(*) * 100, 2) as error_rate
FROM system_logs
WHERE source = 'reddit_scraper'
  AND timestamp > NOW() - INTERVAL '1 hour'
GROUP BY 1
HAVING COUNT(CASE WHEN level = 'error' THEN 1 END)::NUMERIC / COUNT(*) > 0.1; -- Alert if >10% errors
```

## Troubleshooting Guide

### Common Issues and Solutions

```json
{
  "issues": {
    "no_logs": {
      "symptoms": ["No entries in system_logs"],
      "check": "SELECT COUNT(*) FROM system_logs WHERE source = 'reddit_scraper'",
      "solutions": [
        "Verify Supabase connection",
        "Check SUPABASE_SERVICE_ROLE_KEY",
        "Ensure system_logs table exists"
      ]
    },
    "high_rate_limits": {
      "symptoms": ["Many 429 errors in logs"],
      "check": "Check rate_limit logs with monitoring query #4",
      "solutions": [
        "Increase delay between requests",
        "Reduce MAX_THREADS",
        "Check proxy rotation"
      ]
    },
    "zero_metrics": {
      "symptoms": ["engagement = 0 for Ok subreddits"],
      "check": "Check metrics logs with monitoring query #3",
      "solutions": [
        "Verify weekly posts are being fetched",
        "Check if subreddit has recent activity",
        "Review calculator.py logic"
      ]
    },
    "scraper_stopped": {
      "symptoms": ["No recent logs"],
      "check": "Check last activity with alert query",
      "solutions": [
        "Check system_control.enabled",
        "Restart continuous_v3.py",
        "Review error logs before stoppage"
      ]
    }
  }
}
```

## Log Retention & Cleanup

```sql
-- Clean up old logs (run daily)
DELETE FROM system_logs
WHERE timestamp < NOW() - INTERVAL '30 days'
  AND source = 'reddit_scraper';

-- Archive important errors before cleanup
INSERT INTO system_logs_archive
SELECT * FROM system_logs
WHERE level = 'error'
  AND timestamp < NOW() - INTERVAL '30 days'
  AND source = 'reddit_scraper';
```

## Performance Impact

```json
{
  "overhead": {
    "per_log_entry": "~5ms",
    "total_impact": "<2% of runtime",
    "database_storage": "~500 bytes per entry",
    "daily_volume": "~50,000 entries",
    "monthly_storage": "~750MB"
  },
  "optimizations": {
    "batch_logging": "Not implemented (future enhancement)",
    "async_logging": "Uses fire-and-forget pattern",
    "error_handling": "Silently fails to prevent disruption"
  }
}
```

---

_Logging System Version: 1.0.0 | Implementation Date: 2025-01-29 | Status: ACTIVE_