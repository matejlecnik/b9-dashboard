#!/usr/bin/env python3
"""
Startup script for B9 Dashboard API
Runs the API server and optionally starts the scraper if enabled in database
"""

import asyncio
import logging
import os
import signal
import subprocess
import sys
from datetime import datetime, timezone


# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def check_and_start_scrapers():
    """Check database and start both Reddit and Instagram scrapers if enabled"""
    try:
        from supabase import create_client

        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

        if not supabase_url or not supabase_key:
            logger.warning("‚ö†Ô∏è Supabase not configured, skipping scraper auto-start check")
            return

        supabase = create_client(supabase_url, supabase_key)

        # Check and start Reddit scraper
        try:
            result = (
                supabase.table("system_control")
                .select("*")
                .eq("script_name", "reddit_scraper")
                .execute()
            )

            if result.data and len(result.data) > 0 and result.data[0].get("enabled"):
                logger.info("üîÑ Reddit scraper is enabled in database, starting subprocess...")

                # Open log file for Reddit scraper output
                try:
                    reddit_log = open(  # noqa: SIM115 - Must stay open for subprocess
                        "/tmp/reddit_scraper.log", "w"
                    )

                    # Start with output to log file so we can see errors
                    reddit_process = subprocess.Popen(
                        [sys.executable, "-u", "app/scrapers/reddit/reddit_controller.py"],
                        stdout=reddit_log,
                        stderr=subprocess.STDOUT,
                        stdin=subprocess.DEVNULL,
                        start_new_session=True,  # Detach from parent
                        cwd="/app/b9dashboard/backend",  # Ensure correct working directory
                    )

                    # Check if process is still running after a brief moment
                    await asyncio.sleep(2)  # Non-blocking async sleep

                    if reddit_process.poll() is None:
                        # Process is still running
                        logger.info(f"‚úÖ Reddit scraper auto-started with PID: {reddit_process.pid}")

                        # Update heartbeat in database
                        supabase.table("system_control").update(
                            {
                                "last_heartbeat": datetime.now(timezone.utc).isoformat(),
                                "status": "running",
                                "updated_by": "auto_start",
                            }
                        ).eq("script_name", "reddit_scraper").execute()

                    else:
                        # Process died immediately, read error from log
                        reddit_log.close()
                        with open("/tmp/reddit_scraper.log") as f:
                            error_output = f.read()
                        logger.error(f"‚ùå Reddit scraper died immediately. Error: {error_output}")

                        # Mark as stopped in database
                        supabase.table("system_control").update(
                            {
                                "enabled": False,
                                "status": "stopped",
                                "last_error": f"Failed to start: {error_output[:500]}",
                                "updated_by": "auto_start",
                            }
                        ).eq("script_name", "reddit_scraper").execute()

                finally:
                    pass  # Cleanup if needed

            else:
                logger.info("üí§ Reddit scraper is disabled in database, not starting")
        except Exception as e:
            logger.error(f"‚ùå Error checking Reddit scraper: {e}")

        # Check and start Instagram scraper
        try:
            result = (
                supabase.table("system_control")
                .select("*")
                .eq("script_name", "instagram_scraper")
                .execute()
            )

            if result.data and len(result.data) > 0 and result.data[0].get("enabled"):
                logger.info("üîÑ Instagram scraper is enabled in database, starting subprocess...")

                # Open log file for Instagram scraper output
                try:
                    instagram_log = open(  # noqa: SIM115 - Must stay open for subprocess
                        "/tmp/instagram_scraper.log", "w"
                    )

                    # Start with output to log file so we can see errors
                    instagram_process = subprocess.Popen(
                        [sys.executable, "-u", "app/scrapers/instagram/instagram_controller.py"],
                        stdout=instagram_log,
                        stderr=subprocess.STDOUT,
                        stdin=subprocess.DEVNULL,
                        start_new_session=True,  # Detach from parent
                        cwd="/app/b9dashboard/backend",  # Ensure correct working directory
                    )

                    # Check if process is still running after a brief moment
                    await asyncio.sleep(2)  # Non-blocking async sleep

                    if instagram_process.poll() is None:
                        # Process is still running
                        logger.info(
                            f"‚úÖ Instagram scraper auto-started with PID: {instagram_process.pid}"
                        )

                        # Update heartbeat in database
                        supabase.table("system_control").update(
                            {
                                "last_heartbeat": datetime.now(timezone.utc).isoformat(),
                                "status": "running",
                                "updated_by": "auto_start",
                            }
                        ).eq("script_name", "instagram_scraper").execute()

                    else:
                        # Process died immediately, read error from log
                        instagram_log.close()
                        with open("/tmp/instagram_scraper.log") as f:
                            error_output = f.read()
                        logger.error(f"‚ùå Instagram scraper died immediately. Error: {error_output}")

                        # Mark as stopped in database
                        supabase.table("system_control").update(
                            {
                                "enabled": False,
                                "status": "stopped",
                                "last_error": f"Failed to start: {error_output[:500]}",
                                "updated_by": "auto_start",
                            }
                        ).eq("script_name", "instagram_scraper").execute()

                finally:
                    pass  # Cleanup if needed

            else:
                logger.info("üí§ Instagram scraper is disabled in database, not starting at startup")
        except Exception as e:
            logger.error(f"‚ùå Error checking Instagram scraper: {e}")

    except Exception as e:
        logger.error(f"‚ùå Error checking scraper auto-start: {e}")


def run_api():
    """Run the FastAPI server with production or development configuration"""
    logger.info("üöÄ Starting FastAPI server...")
    port = os.environ.get("PORT", "8000")
    environment = os.environ.get("ENVIRONMENT", "development")

    try:
        # Add current directory to Python path
        sys.path.insert(0, "/app")
        os.environ["PYTHONPATH"] = "/app"

        if environment == "production":
            # Use Gunicorn + Uvicorn workers for production (multi-worker)
            logger.info("üè≠ Running in PRODUCTION mode (Gunicorn + Uvicorn workers)")
            from production_server import run_production_server

            run_production_server()
        else:
            # Use standalone Uvicorn for development (single worker, auto-reload)
            logger.info("üõ†Ô∏è  Running in DEVELOPMENT mode (Uvicorn single worker)")
            import uvicorn

            from main import app

            uvicorn.run(
                app,
                host="0.0.0.0",
                port=int(port),
                reload=True,  # Enable auto-reload in development
                log_level="debug",
            )
    except Exception as e:
        logger.error(f"‚ùå API server crashed: {e}")


def signal_handler(sig, frame):
    """Handle shutdown signals"""
    logger.info("\nüõë Shutting down services gracefully...")
    sys.exit(0)


if __name__ == "__main__":
    # Handle shutdown signals
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    logger.info("üöÄ Starting B9 Dashboard Services...")
    logger.info(f"üìã Environment: {os.environ.get('ENVIRONMENT', 'unknown')}")
    logger.info(f"üåê Port: {os.environ.get('PORT', '8000')}")

    # Cleanup script removed - not needed in simplified architecture
    # Can be re-added if cleanup_old_files.py is created

    # Check if scrapers should auto-start based on database state
    logger.info("üîç Checking if scrapers should auto-start...")
    asyncio.run(check_and_start_scrapers())

    # Run API server in main thread
    logger.info("üéÅ Starting API server in main thread...")
    run_api()
